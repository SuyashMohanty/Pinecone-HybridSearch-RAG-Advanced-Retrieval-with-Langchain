# üå≤ Pinecone HybridSearch RAG: Advanced Retrieval with Langchain

**Pinecone HybridSearch RAG** demonstrates a Retrieval Augmented Generation (RAG) system that leverages **Hybrid Search** with a **Pinecone Vector Database** and **Langchain**. This project showcases how to combine semantic (dense) search with keyword (sparse) search to improve the relevance of retrieved documents for more accurate and contextually rich LLM responses. It utilizes Sentence Transformers for generating dense embeddings and Pinecone's capabilities for efficient hybrid search.

---
## üß† Understanding Hybrid Search

**Hybrid Search** is an advanced information retrieval technique that combines the strengths of two distinct search methodologies:

1.  **Semantic Search (Dense Retrieval)**:
    * **How it works**: This method uses dense vector embeddings (generated by models like Sentence Transformers) to capture the *meaning* and *context* of the query and the documents. It excels at finding documents that are conceptually similar, even if they don't share exact keywords.
    * **Pros**: Great for understanding nuanced queries, synonyms, and related concepts.
    * **Cons**: Can sometimes miss documents that have exact keyword matches but slightly different contextual embeddings, or might over-generalize.

2.  **Keyword Search (Sparse Retrieval / Lexical Search)**:
    * **How it works**: This traditional method relies on matching exact keywords or their variations (e.g., using algorithms like BM25 or TF-IDF). It focuses on the presence and frequency of specific terms.
    * **Pros**: Excellent for queries where specific terms, jargon, or identifiers are crucial. Ensures that documents containing these explicit keywords are ranked highly.
    * **Cons**: Can struggle with synonyms, paraphrasing, or understanding the broader context if the exact keywords are not present.

**The Power of Hybrid:**
By combining these two approaches, Hybrid Search aims to provide a more robust and relevant search experience. A weighting mechanism (often an `alpha` parameter, where `alpha` ranges from 0 to 1) is typically used to balance the influence of semantic and keyword search scores:
* `alpha = 1.0`: Purely semantic search.
* `alpha = 0.0`: Purely keyword search.
* `alpha` between 0 and 1: A weighted combination of both.

This allows the system to retrieve documents that are both semantically related *and* contain important keywords, leading to better context for the RAG system.

---
## üöÄ Features

* **Hybrid Search Implementation**: Demonstrates hybrid search combining dense (semantic) and sparse (keyword) vectors.
* **Pinecone Vector Database**: Utilizes Pinecone for storing and efficiently querying both dense and sparse vectors.
* **Langchain Integration**: Leverages Langchain for orchestrating the retrieval process.
* **Sentence Transformers**: Uses `sentence-transformers/all-MiniLM-L6-v2` for generating dense vector embeddings.
* **Sparse Vector Generation**: Implicitly handles sparse vector creation suitable for Pinecone (Pinecone often uses its own methods like SPLADE or relies on TF-IDF like structures for sparse parts).
* **Custom Retriever**: Shows the setup of a Pinecone retriever capable of hybrid search.
* **Example Workflow**: Provides a clear example of initializing Pinecone, creating an index, adding text data (with both dense and sparse representations), and performing hybrid search queries.

---
## ‚öôÔ∏è How it Works (Conceptual Flow)

1.  **Initialization**:
    * Initialize connection to Pinecone using API key and environment.
    * Define the Pinecone index name.
2.  **Embedding Models**:
    * Set up the dense embedding model (e.g., `sentence-transformers/all-MiniLM-L6-v2`).
    * (Implicit) Prepare for sparse vector representation (Pinecone handles the specifics of how sparse data is encoded and indexed).
3.  **Indexing**:
    * For each document/text chunk:
        * Generate a dense vector embedding using the Sentence Transformer.
        * Generate or prepare data for sparse vector representation.
        * Upsert these to Pinecone, associating them with the original text and a unique ID. The notebook uses `PineconeVectorStore.from_texts` which handles this process.
4.  **Retrieval (Hybrid Search)**:
    * When a query comes in:
        * Generate a dense vector for the query using the same Sentence Transformer.
        * Prepare the query for sparse matching.
        * The Langchain `PineconeVectorStore` retriever (configured for hybrid search or if hybrid is a default/configurable mode for the used Pinecone client version) sends these to Pinecone.
        * Pinecone performs a hybrid search, calculating scores based on both dense vector similarity and sparse vector matches, and combines them (often using an alpha weighting).
    * The retriever returns the most relevant documents based on the combined hybrid score.
5.  **RAG (Next Step)**:
    * (Conceptual) The retrieved documents would then be passed to a Large Language Model (LLM) along with the original query to generate an informed answer. This part is not explicitly implemented in the provided notebook but is the typical next step in a RAG pipeline.

---
## üìã Requirements

* Python 3.x
* Jupyter Notebook or an IDE that supports `.ipynb` files
* Pinecone Account and API Key.
* Required Python libraries: `pinecone-client`, `pinecone-text`, `langchain`, `langchain-pinecone`, `sentence-transformers`, `torch`.

---
## üõ†Ô∏è Setup & Installation

1.  **Clone the repository (if applicable, or download the `.ipynb` file).**

2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    ```

3.  **Install the required Python libraries:**
    The notebook uses `%pip install` for some libraries. For a clean setup, you can install them directly:
    ```bash
    pip install pinecone-client pinecone-text langchain langchain-pinecone sentence-transformers torch
    # The notebook also installs pinecone-notebooks, which is useful for Jupyter environments
    pip install pinecone-notebooks
    ```

4.  **Set up Pinecone:**
    * Sign up for a Pinecone account and get your API key and environment details.
    * The notebook expects the API key to be set in a variable `api_key`. For better practice, consider using environment variables:
        ```python
        import os
        # from dotenv import load_dotenv
        # load_dotenv() # if you store keys in a .env file

        PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
        # PINECONE_ENVIRONMENT = os.getenv("PINECONE_ENVIRONMENT") # If your environment is also needed
        if not PINECONE_API_KEY:
            raise ValueError("Pinecone API key not found. Set the PINECONE_API_KEY environment variable.")
        ```

5.  **Define Pinecone Index:**
    * Choose a name for your Pinecone index. The notebook uses `index_name = 'hybrid-search-test'`.
    * Ensure the index is created in Pinecone with dimensions matching your dense embedding model (e.g., `all-MiniLM-L6-v2` has a dimension of 384) and configured to support hybrid search with sparse vectors if required by your Pinecone setup. The script attempts to create the index if it doesn't exist.

---
## ‚ñ∂Ô∏è Usage

1.  **Open the Jupyter Notebook:**
    ```bash
    jupyter notebook experiment.ipynb
    ```
    Or open it with your preferred IDE.

2.  **Configure Pinecone Credentials**:
    * Ensure your Pinecone API key (and environment, if needed by your client version) is correctly set up in the notebook or via environment variables.

3.  **Run the Cells Sequentially:**
    * **Initial Setup**: Cells install and import necessary libraries.
    * **Pinecone Initialization**: Cells initialize the Pinecone client with your API key and define the index name. It checks if the index exists and creates it if not, specifying dimensions for dense vectors and configuring for sparse vectors.
    * **Embedding Model**: Initializes the `SentenceTransformer` model for dense embeddings.
    * **Vector Store and Retriever**: Sets up `PineconeVectorStore` from Langchain, which will be used for adding texts and performing retrieval. It's configured with the Pinecone index, embedding function, and text key.
    * **Adding Data**: Demonstrates adding texts to the Pinecone index. `PineconeVectorStore.from_texts` or `add_texts` will internally generate dense embeddings and prepare sparse representations for Pinecone.
    * **Performing Hybrid Search**: The `retriever.invoke("Your query here")` or similar methods (like `vectorstore.similarity_search_with_score`) are used to query the index. If the Pinecone index and client are correctly set up for hybrid search, this step will perform a hybrid query. You might need to pass specific parameters like `alpha` depending on the Langchain/Pinecone client version if not defaulting to a desired hybrid mode.

4.  **Experiment with Queries and Alpha (if applicable):**
    * Modify the query in the `retriever.invoke()` or `similarity_search` calls to test different scenarios.
    * If your Pinecone client version or Langchain retriever wrapper supports explicit alpha tuning for hybrid search, experiment with different `alpha` values to see how it affects the balance between semantic and keyword search results.

---